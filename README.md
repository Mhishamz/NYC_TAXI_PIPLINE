# ğŸš• NYC Taxi Data Pipeline

> **Production-grade ELT pipeline**: NYC Open Data API â†’ PostgreSQL (Medallion Architecture) â†’ Power BI  
> Orchestrated with **Apache Airflow**, transformed with **dbt**, containerized with **Docker Compose**

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        NYC Taxi ELT Pipeline                        â”‚
â”‚                                                                     â”‚
â”‚  NYC Open Data API                                                  â”‚
â”‚       â”‚                                                             â”‚
â”‚       â–¼                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ Airflow  â”‚â”€â”€â”€â–¶â”‚          PostgreSQL Data Warehouse           â”‚   â”‚
â”‚  â”‚Scheduler â”‚    â”‚                                             â”‚   â”‚
â”‚  â”‚  (DAG)   â”‚    â”‚  bronze.*  â”‚  silver.*  â”‚     gold.*        â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚  (raw)     â”‚  (cleaned) â”‚  (aggregated)     â”‚   â”‚
â”‚       â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚       â”‚                    â–²                                        â”‚
â”‚       â””â”€â”€â”€â”€ dbt â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                        â”‚
â”‚              (bronzeâ†’silverâ†’gold transformations + tests)           â”‚
â”‚                                                                     â”‚
â”‚  metadata.pipeline_runs (audit log)          Power BI â—€â”€â”€ gold.*   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Medallion Architecture

| Layer      | Schema     | Table                     | Description                          |
|------------|------------|---------------------------|--------------------------------------|
| ğŸ¥‰ Bronze  | `bronze`   | `raw_taxi_trips`          | Raw strings from API, append-only    |
| ğŸ¥ˆ Silver  | `silver`   | `cleaned_taxi_trips`      | Typed, validated, deduplicated       |
| ğŸ¥‡ Gold    | `gold`     | `daily_trip_summary`      | Daily business aggregations          |
| ğŸ¥‡ Gold    | `gold`     | `hourly_demand_heatmap`   | Hour Ã— DOW demand matrix             |
| ğŸ¥‡ Gold    | `gold`     | `payment_analysis`        | Payment type breakdown               |
| ğŸ“‹ Meta    | `metadata` | `pipeline_runs`           | Pipeline audit log                   |

---

## ğŸ³ Services

| Service            | Port   | Description                          |
|--------------------|--------|--------------------------------------|
| Airflow Webserver  | 8080   | DAG monitoring & triggering          |
| postgres-airflow   | â€”      | Airflow metadata (internal)          |
| postgres-dw        | 5433   | NYC Taxi Data Warehouse              |
| Adminer            | 8081   | Database GUI (dev use)               |

---

## ğŸš€ Quick Start

### 1. Clone & configure

```bash
git clone <repo-url>
cd nyc-taxi-pipeline
cp .env.example .env   # edit if needed
```

### 2. Start the stack

```bash
docker compose up -d --build
```

### 3. Register Airflow connections

```bash
chmod +x scripts/setup_connections.sh
./scripts/setup_connections.sh
```

### 4. Install dbt packages & test connection

```bash
# One-off dbt container run
docker compose --profile dbt run dbt deps
docker compose --profile dbt run dbt debug
```

### 5. Trigger the pipeline

- Open **http://localhost:8080** (admin / admin)
- Enable the `nyc_taxi_pipeline` DAG
- Click **Trigger DAG**

### 6. Connect Power BI

See [`powerbi/POWERBI_SETUP.md`](powerbi/POWERBI_SETUP.md) for full instructions.  
Short version: connect to `localhost:5433 / nyc_taxi_dw` â†’ load `gold.*` tables.

---

## ğŸ“ Project Structure

```
nyc-taxi-pipeline/
â”œâ”€â”€ docker-compose.yml              # All services
â”œâ”€â”€ .env                            # Environment variables
â”‚
â”œâ”€â”€ airflow/
â”‚   â”œâ”€â”€ dags/
â”‚   â”‚   â””â”€â”€ nyc_taxi_pipeline.py   # Main pipeline DAG
â”‚   â”œâ”€â”€ logs/                       # Airflow task logs
â”‚   â”œâ”€â”€ plugins/
â”‚   â””â”€â”€ config/airflow.cfg
â”‚
â”œâ”€â”€ dbt/
â”‚   â”œâ”€â”€ dbt_project.yml
â”‚   â”œâ”€â”€ profiles.yml
â”‚   â”œâ”€â”€ packages.yml               # dbt_utils
â”‚   â””â”€â”€ models/
â”‚       â”œâ”€â”€ bronze/
â”‚       â”‚   â””â”€â”€ sources.yml        # Source definitions + tests
â”‚       â”œâ”€â”€ silver/
â”‚       â”‚   â”œâ”€â”€ cleaned_taxi_trips.sql
â”‚       â”‚   â””â”€â”€ schema.yml         # Column-level tests
â”‚       â””â”€â”€ gold/
â”‚           â”œâ”€â”€ daily_trip_summary.sql
â”‚           â”œâ”€â”€ hourly_demand_heatmap.sql
â”‚           â”œâ”€â”€ payment_analysis.sql
â”‚           â””â”€â”€ schema.yml
â”‚   â””â”€â”€ tests/                     # Custom singular tests
â”‚       â”œâ”€â”€ assert_dropoff_after_pickup.sql
â”‚       â”œâ”€â”€ assert_total_amount_gte_fare.sql
â”‚       â””â”€â”€ assert_gold_coverage.sql
â”‚
â”œâ”€â”€ postgres/
â”‚   â””â”€â”€ init/
â”‚       â””â”€â”€ 01_init_schemas.sql    # Schema + table DDL
â”‚
â”œâ”€â”€ powerbi/
â”‚   â””â”€â”€ POWERBI_SETUP.md           # Connection + dashboard guide
â”‚
â””â”€â”€ scripts/
    â””â”€â”€ setup_connections.sh       # Post-startup Airflow config
```

---

## ğŸ” DAG Flow

```
log_pipeline_start
      â”‚
      â–¼
 ingest_data            â† Fetches NYC API, loads to bronze.*
      â”‚
      â–¼
validate_bronze         â† Null checks, row counts
      â”‚
      â–¼
dbt_transform_silver    â† bronze â†’ silver (cast, clean, dedupe)
      â”‚
      â–¼
dbt_transform_gold      â† silver â†’ gold (aggregate for BI)
      â”‚
      â–¼
   dbt_test             â† Schema tests + custom singular tests
      â”‚
      â–¼
dbt_generate_docs       â† Refreshes dbt catalog
      â”‚
   â”Œâ”€â”€â”´â”€â”€â”
   â–¼     â–¼
 log_  log_
success failure
```

---

## ğŸ§ª dbt Tests

### Generic (schema.yml)
- `unique`, `not_null` on all key columns
- `accepted_values` on vendor_id, payment_type
- `dbt_utils.expression_is_true` for numeric range checks (fares > 0, tip rate 0â€“1)

### Singular (tests/)
| Test                              | Assertion                                      |
|-----------------------------------|------------------------------------------------|
| `assert_dropoff_after_pickup`     | No trips with negative duration               |
| `assert_total_amount_gte_fare`    | Total â‰¥ base fare always                       |
| `assert_gold_coverage`            | Every silver date exists in gold              |

---

## ğŸ“Š Silver Layer Transformations

| Raw Column              | Silver Column            | Transformation                        |
|-------------------------|--------------------------|---------------------------------------|
| `passenger_count` (str) | `passenger_count` (INT)  | Cast + null filter                    |
| `fare_amount` (str)     | `fare_amount` (NUMERIC)  | Cast + validate â‰¥ 0                   |
| `payment_type` (str)    | `payment_type_label`     | Code â†’ label map                      |
| â€”                       | `trip_duration_mins`     | EXTRACT(EPOCH from dropoff-pickup)/60 |
| â€”                       | `avg_speed_mph`          | distance / duration                   |
| â€”                       | `cost_per_mile`          | total / distance                      |
| â€”                       | `tip_percentage`         | tip / total                           |
| â€”                       | `trip_id`                | SHA surrogate key (dbt_utils)         |

---

## ğŸ“ˆ Monitoring & Logging

- **Pipeline runs** logged in `metadata.pipeline_runs` with row counts and duration
- **Airflow task logs** persisted in `airflow/logs/` (JSON format from dbt)
- **Docker logs**: `docker compose logs -f airflow-scheduler`
- **Adminer**: http://localhost:8081 for live SQL queries on all schemas

---

## ğŸ”§ Configuration

| Variable        | Default                                           | Description          |
|-----------------|---------------------------------------------------|----------------------|
| `PAGE_SIZE`     | 1000                                              | Records per API page |
| `MAX_PAGES`     | 10                                                | Max pages per run    |
| `NYC_TAXI_URL`  | https://data.cityofnewyork.us/resource/qp3b-zxtp.json | Source URL    |
| Schedule        | `0 2 * * *`                                       | Daily 02:00 UTC      |

---

## ğŸ› ï¸ Common Commands

```bash
# View all logs
docker compose logs -f

# Run dbt manually
docker compose --profile dbt run dbt run --select silver
docker compose --profile dbt run dbt test

# Inspect warehouse
docker compose exec postgres-dw psql -U dwuser -d nyc_taxi_dw \
  -c "SELECT COUNT(*) FROM silver.cleaned_taxi_trips;"

# Check pipeline metadata
docker compose exec postgres-dw psql -U dwuser -d nyc_taxi_dw \
  -c "SELECT * FROM metadata.pipeline_runs ORDER BY run_id DESC LIMIT 5;"

# Stop everything
docker compose down -v   # -v removes volumes (fresh start)
```

---

## ğŸ”’ Production Hardening Checklist

- [ ] Move credentials to **Docker Secrets** or **Vault**
- [ ] Enable **remote logging** (S3/GCS) in `airflow.cfg`
- [ ] Add **AlertManager** or email alerts on DAG failure
- [ ] Set `MAX_PAGES` dynamically based on last ingestion watermark
- [ ] Add **Incremental loads** (filter by `tpep_pickup_datetime > last_run`)
- [ ] Enable **dbt Cloud** for hosted docs + job scheduling
- [ ] Add **Great Expectations** for advanced data quality profiling
# NYC_TAXI_PIPLINE
